<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research</title>
    <style>
        /* Reset CSS */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f9;
            color: #333;
            line-height: 1.6;
        }
        h1, h3, h4 {
            color: #333;
        }

        /* Navigation Bar */
        #navbar {
            list-style: none;
            display: flex;
            justify-content: center;
            background-color: #333;
            padding: 12px 0;
            margin: 0;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            width: 100%;
        }
        #navbar li {
            margin: 0 25px;
        }
        #navbar a {
            color: white;
            text-decoration: none;
            font-size: 18px; /* Increased font size */
            font-weight: 500;
            padding: 8px 16px;
            display: inline-block;
            transition: background-color 0.3s;
        }
        #navbar a:hover, .home-icon:hover,
        .literature-icon:hover,
        .research-icon:hover,
        .blog-icon:hover,
        .about-icon:hover {
            background-color: #007bff;
            border-radius: 4px;
           
        }
        .home-icon {
            color: white; /* Blue color */
            font-size: 24px; /* Size of the icon */
            transition: color 0.3s, transform 0.3s;
        }
        .about-icon {
            color: white; /* Blue color */
            font-size: 24px; /* Icon size */
            transition: color 0.3s, transform 0.3s;
        }
        .literature-icon {
            color: white; /* Blue color */
            font-size: 24px; /* Icon size */
            transition: color 0.3s, transform 0.3s;
        }

        .research-icon {
            color: white; /* Blue color */
            font-size: 24px; /* Icon size */
            transition: color 0.3s, transform 0.3s;
        }
        .blog-icon {
            color: white; /* Green color */
            font-size: 24px; /* Icon size */
            transition: color 0.3s, transform 0.3s;
        }

        


        /* Page Content */
        #content {
            padding: 30px 5%;
        }

        /* Research Papers Section */
        .research-paper-section {
            margin-top: 30px;
            padding: 20px;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        .research-paper-section h3 {
            font-size: 22px;
            margin-bottom: 15px;
            font-weight: 800;
            color: teal;
        }
        .research-paper-container {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
        }
        .research-paper-category {
            width: 30%;
            margin-bottom: 15px;
        }
        .research-paper-category h4 {
            font-size: 18px;
            margin-bottom: 10px;
            font-weight: 800;
            color: #007bff;
        }
        .research-paper-category ol {
            margin-left: 30px;
            margin-right: 30px;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .research-paper-category li {
            font-size: 16px;
            line-height: 1.6;
            margin-bottom: 8px;
            color: #007bff;
        }
        .research-paper-category strong {
            color: teal;
        }
        .research-paper-category i {
            color:red
        }
        .research-paper-category a i {
            color:blue;
        }
        .button {
            display: inline-block;
            padding: 2px 6px;
            background:white;
            color: blue;
            font-size: .6rem;
            font-weight: 200;
            text-decoration: none;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            transition: background 0.3s, transform 0.3s, box-shadow 0.3s;
        }
        .button:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.3);
            
        }
        
        /* PPTs and Codes */
        #PCR {
            display: flex;
            justify-content: space-between;
            gap: 20px;
            margin-top: 30px;
        }

        .ppts, .codes, .references {
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            flex: 1;
        }

        .ppts h3, .codes h3, .references h3 {
            font-size: 20px;
            font-weight: 800;
            margin-bottom: 16px;
            color:teal;
        }

      
        .codes ul, .references ul, .ppts ul {
            list-style-type: disc;
            padding-left: 20px;
        }

        .codes li, .references li, .ppts li {
            margin-bottom: 10px;
            font-size: 16px;
            line-height: 1.5;
            color: #333;
        }

        .whatsapp-icon {
        font-size: 16px;
        color: #25d366; /* WhatsApp green */
        text-decoration: none;
        transition: transform 0.2s ease;
        }

        .whatsapp-icon:hover {
        color: #25d366;
        }
        .email-icon {
        font-size: 16px;
        color: #333; /* Default color */
        text-decoration: none;
        transition: color 0.3s ease;
        }

        .email-icon:hover {
        color: #007bff; /* Hover color */
        }
        .linkedin-icon {
        font-size: 16px;
        color: #0a66c2; /* LinkedIn blue */
        text-decoration: none;
        transition: color 0.3s ease;
        }

        .linkedin-icon:hover {
        color: #004182; /* Darker blue on hover */
        }
        .github-icon {
        font-size: 16px;
        color: #333; /* GitHub logo color */
        text-decoration: none;
        transition: color 0.3s ease;
        }

        .github-icon:hover {
        color: #4078c0; /* Hover effect */
        }
        
        
        /* Footer */
        #footer {
            background-color: #333;
            color: white;
            text-align: center;
            padding: 10px 5%;
            font-size: 14px;
            margin-top: 10px;
            border-top: 1px solid #007bff;
            line-height: .6;
        }

        .footer-content p {
            margin-bottom: 10px;
            font-size: 16px;
        }

        .footer-links a {
            color: #007bff;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }

        .footer-links a:hover {
            color: #0056b3;
        }

        .footer-links {
            margin-top: 10px;
            font-size: 16px;
        }

    </style>
    <!-- Include Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>

    <!-- Navbar -->
    <ul id="navbar">
        <li><a href="index.html"><i class="fas fa-home home-icon"></i> Home</a></li>
        <li><a href="about.html"> <i class="fas fa-info-circle about-icon"></i> About</a></li>
        <li><a href="literature.html"><i class="fas fa-book literature-icon"></i> Literature</a></li>
        <li><a href="research.html"><i class="fas fa-search research-icon"></i> Research</a></li>
        <li><a href="blogs.html"> <i class="fas fa-pencil-alt blog-icon"></i> Blogs & Reviews</a></li>
    </ul>

    <!-- Main Content -->
    <div id="content">


        <!-- Research Paper Section -->
        <div class="research-paper-section">
            <h3>Research Papers</h3>
            <div class="research-paper-container">
                <!-- Natural Language Processing -->
                <div class="research-paper-category">
                    <h4>Natural Language Processing</h4>
                    <ol>
                        <li>
                            <strong>
                                 Sequence to Sequence Learning with Neural Networks   
                            </strong>
                            <i> 
                                Ilya Sutskever et al.
                            </i>
                            <a href="" class="button" download=""><i>Download</i></a>
                            <a href="" class="button" download=""><i>Summary</i></a>      
                        </li>



                        <li><strong>  Long Short Term Memory       </strong><i>  Hochreiter et al.,      </i>      </li>
                        <li><strong>   Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modelling      </strong><i>  Chung et al.,      </i>      </li>
                        <li><strong> On the Properties of Neural Machine Translation: Encoder-Decoder Approaches         </strong><i>   Cho et al.,     </i>      </li>
                        <li><strong>  Boosting Image Captioning with Attributes       </strong><i>  Yao et al.,      </i>      </li>
                        <li><strong> Long Short Term Memory for Machine Reading        </strong><i>  Cheng et al.,      </i>      </li>
                        <li><strong> Neural Machine Translation by Jointly Learning to Align and Translate        </strong><i>  Bahdanau et al.,      </i>  ICLR 2015    </li>
                        <li><strong>  BLEU Score: a Method for Automatic Evaluation of Machine Translation       </strong><i>  Papineni at al.,      </i>  ACL 2002    </li>
                        <li><strong>  Sequence to Sequence Learning with Neural Networks       </strong><i>  Sutskever et al.,       </i> NeurIPS 2014     </li>
                        <li><strong> Semi-supervised Sequence Learning        </strong><i> Dai et al.,        </i>  NeurIPS 2015    </li>
                        <li><strong>    Universal Language Model Fine-Tuning for Text Classification     </strong><i> Howard et al.,       </i>  ACL 2018    </li>
                        <li><strong>   Deep contexualized word representation      </strong><i>  Peters et al.,      </i> ACL 2018     </li>
                        <li><strong> Distributed Representation of Word and Phrases and their Compositionality        </strong><i>   Mikolov et al.,      </i>  NeurIPS 2013    </li>
                        <li><strong>  An Introduction to Vision Language Modelling       </strong><i>  Bordes et al.,       </i> Meta 2024      </li>
                        <li><strong>  Large Language Models: A Survey       </strong><i>   Minaee et al.,     </i> arXive 2024     </li>
                        <li><strong>  Deep Learning Approaches on Image Captioning: A Review       </strong><i>  Gandhi et al.,      </i> arXive 2023     </li>
                        <li><strong> Don't Stop Pretraining: Adapt Language Models to Domains and Tasks        </strong><i> Gururangan et al.,       </i> ACL 2020     </li>
                        <li><strong>    A Simple Framework for Contrastive Learning of Visual Representations     </strong><i>  Chen at al.,      </i>  ICML 2020    </li>
                        <li><strong>  Contrastive Learning of Medical Visual Representation from Paired Images and Text       </strong><i> Zhang et al.,       </i>ICLR 2021      </li>
                        <li><strong>     VirTex Learning Representations from Textual Annotations    </strong><i>  Desai et al.,      </i> CVPR 2020     </li>
                        <li><strong> Efficient Estimation of Word Representations in Vector Space        </strong><i> Mokolov et al.,       </i> 2013      </li>
                        <li><strong>  SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural text Processing       </strong><i>  Kudo et al.,      </i>      </li>
                        <li><strong> Neural Machine Translation of Rare Words with Subword Units         </strong><i>  Sennrich et al.,       </i>      </li>
                        <li><strong>  Google's Neural Machine translation System: Bridging the Gap between Human and Machine Translation       </strong><i> Wu et al.,       </i>      </li>
                        <li><strong>  Learning Transferable Visual Models From Natural Language Supervision       </strong><i>  Radford et al.,       </i> ICML 2021     </li>
                        <li><strong>   ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks      </strong><i>   Lu et al.,     </i>      </li>
                        <li><strong>  Swin Transformer: Hierarchical Vision Transformer using Shifted Windows       </strong><i>  Liu et al.,      </i>   ICCV 2021   </li>
                        <li><strong>  VisualGPT: Data-efficient Adaption of Pretrained Language Models for Image Captioning       </strong><i>  Chen et al.,      </i> CVPR 2022     </li>
                        <li><strong>  VisualBERT: A Simple and Performant Baseline for Vision and Language       </strong><i> Li et al.,       </i>      </li>
                        <li><strong>  Improving Language Understanding by Generative Pre-Training       </strong><i>  Radford      </i>      </li>
                        <li><strong>  BERT: Pre-training of Deep Bidirectional Trandformers for Language Understanding      </strong><i> Delvin et al.,       </i>  ACL 2018    </li>
                        <li><strong>   BART: Denoiding Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,and Comprehension      </strong><i>  Lewis et al.,      </i>   ACL 2020   </li>
                        <li><strong>   RoBARTa: A Robustly Optimized BERt Pretraining Approach      </strong><i> Liu et al.,       </i>      </li>
                        <li><strong> Exploring the Limits of Transfer learning with a Unified Text-to-Text Transformer        </strong><i>   Raffel et al.,     </i>      </li>
                        <li><strong>   Large Models are Unsupervised Multitask Learners      </strong><i>   Radford et al.,      </i>      </li>
                        <li><strong>  Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks       </strong><i> Reimers et al.,       </i>      </li>
                        <li><strong>  DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter       </strong><i> Sanh et al.,       </i>      </li>
                        <li><strong>  DeBERTa: Decoding-Enhanced BERT with Disentangled Attention       </strong><i> He at al.,       </i>      </li>
                        <li><strong> DeBERTa v3: Improving DeBERTa using ELECTRA-style Pretraining with Gradient Disentangled Embedding Sharing        </strong><i> He et al.,       </i>      </li>
                        <li><strong>  GLU Variants Improve Transformers       </strong><i> Shazeer et al.,       </i>      </li>
                        <li><strong>   CLIPText: A New Paradigm for Zero-Shot Text Classification      </strong><i>     Qin et al.,   </i>      </li>
                        <!-- <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                         -->
                        
                    </ol>
                </div>
                <div class="research-paper-category">
                    <h4>Computer Vision</h4>
                    <ol>
                        <li><strong>Histogram of Oriented Gradients for Human Detection </strong><i>Dalal et al., </i>      </li>
                        <li><strong>Rapid Object Detection using Gradient Boosted Cascade of Simple Features </strong><i>Viola et al.,        </i>      </li>
                        <li><strong>  Mask R-CNN       </strong><i> He et al.,       </i>      </li>
                        <li><strong>  Deep Lab       </strong><i>  Chen et al.,      </i>      </li>
                        <li><strong> Hollywood in Homes        </strong><i> Sigurdsson et al.,        </i>      </li>
                        <li><strong> Fully Convolutional Network for Semantic Segmentation   </strong><i> Long et al.,        </i>      </li>
                        <li><strong>    Show,Attend and Tell: Neural Image Caption Generation with Visual Attention     </strong><i> Xu at al.,       </i>   ICML 2015   </li>
                        <li><strong> Deep Visual-Semantic Alignment for generating Image Description        </strong><i> Karpathy et al.,        </i>      </li>
                        <li><strong>   3D Convolutional Neural Networks for Human Action Recognition      </strong><i> Ji et al.,        </i>      </li>
                        <li><strong> Neural Baby Talk        </strong><i>  Lu at al.,      </i>  CVPR    </li>
                        <li><strong> Long Term Recurrent Convolutional Network for Visual Recognitiona and Description        </strong><i>  Donahue et al.,      </i>      </li>
                        <li><strong>  Show and Tell: A Neural Image Caption Generator       </strong><i> Vinyals et al.,       </i>      </li>
                        <li><strong>  U-Net: Convolutional Networks for Biomedical Image Segmentation       </strong><i> Ronneberger et al.,       </i>      </li>
                        <li><strong> Visualizing and Understanding Convolutionsal Networks        </strong><i> Zeiler et al.,       </i>  ECCV 2013    </li>
                        <li><strong>   MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications      </strong><i> Howard et al.,       </i>      </li>
                        <li><strong>  Gradient-Based Learing Applied to Document Recognition      </strong><i> LeCunn et al.,       </i> 1998     </li>
                        <li><strong>  Deep Residual Learning for Image Recognition       </strong><i> He et al.,       </i>      </li>
                        <li><strong>   Going Deeper with Convolutions      </strong><i> Szegedy et al.,       </i>      </li>
                        <li><strong> Very Deep Convolutional Networks for Large Scale Image Recognition        </strong><i> Simonyan et al.,       </i> 2015     </li>
                        <li><strong> Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification        </strong><i>  He et al.,      </i>      </li>
                        <li><strong> ImageNet Classification with Deep Convolutional Neural Networks        </strong><i> Krizhevsky et al.,       </i> 2012     </li>
                        <!-- <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li>
                        <li><strong>         </strong><i>        </i>      </li> -->
                        
                    </ol>
                </div>
                <div class="research-paper-category">
                    <h4>Deep Learning</h4>
                    <ol>
                        <li>The Road to Modern AI</li>
                        <li>The Summer Vision Project</li>
                        <!-- <li></li>
                        <li></li>
                        <li></li>
                        <li></li>
                        <li></li>
                        <li></li>
                        <li></li>
                        <li></li> -->
                    </ol>
                </div>
                <!-- <div class="research-paper-category">
                    <h4>Programming</h4>
                    <ul>
                        <li>Python, PyTorch</li>
                        <li>C/C++</li>
                        <li>HuggingFace</li>
                        <li>LaTeX</li>
                        <li>Git and GitHub</li>
                    </ul>
                </div> -->

                
            </div>
        </div>

        


        <!-- PPTs-Codes-References Section -->
        <div id="PCR">
            <!-- PPTs section -->
            <div class="ppts">
                <h3>PPTs</h3>
                <ul>
                    <li>Deep Learning (Jan 10, 2024) <a href="#" target="_blank" download> Download</a>
                    </li>
                    <li>Machine Learning (Jan 10, 2024) <a href="#" target="_blank" download> Download</a>
                    </li>
                    <li>Optimization (Jan 10, 2024) <a href="#" target="_blank" download> Download</a>
                    </li>
                </ul>
            </div>


            <!-- Codes Section -->
            <div class="codes">
                <h3>Codes</h3>
                <ul>
                    <li>Transformer <a href="#" target="_blank" download> Download</a>
                    </li>
                    <li>CLIP (Jan 10, 2024) <a href="#" target="_blank" download> Download</a>
                    </li>
                    <li>Word Embeddings (Jan 10, 2024) <a href="#" target="_blank" download> Download</a>
                    </li>
                </ul>
            </div>

            <!-- References Section -->
            <div class="references">
                <h3>References</h3>
                <ul>
                    <li>Natural Language Processing with Deep Learning, Stanford 
                        <a href="https://web.stanford.edu/class/cs224n/index.html#schedule"
                            target="_blank" download>
                             <i class="fas fa-external-link-alt"></i>
                        </a>
                    </li>
                    <li>Deep Learning for Computer Vision, Stanford 
                        <a href="https://cs231n.stanford.edu/"
                            target="_blank" download>
                             <i class="fas fa-external-link-alt"></i>
                        </a>
                    </li>
                    <li>Deep Learning for Computer Vision, University of Michigan 
                        <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/schedule.html"
                            target="_blank" download>
                             <i class="fas fa-external-link-alt"></i>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>


        


    </div>
    <!-- Footer -->
<footer id="footer">
    <div class="footer-content">
        <p>Website copyright © 2025. Developed and maintained by <strong>Shankul Patel</strong>.</p>
        <p>This website is under development.</p>
    </div>
    <div class="footer-links">
        <a href="https://www.linkedin.com/in/spatel2804" target="_blank" class="linkedin-icon">
            <i class="fa-brands fa-linkedin"></i>
        </a> | 

        <a href="mailto:shanku62_scs@gmail.com" class="email-icon">
            <i class="fa-solid fa-envelope"></i>
        </a> |

        <a href="https://wa.me/9569770695" target="_blank" class="whatsapp-icon">
            <i class="fa-brands fa-whatsapp"></i>
        </a>
    </div>
</footer>


</body>
</html>
